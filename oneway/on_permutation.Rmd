---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Unbalanced two-way ANOVA

This page follows on from the [two-way unbalanced ANOVA
notebook](./twoway_unbalanced.ipynb).

Please make sure you follow the two-way unbalanced ANOVA notebook before you read this notebook, because we are going to re-use notation and machinery
from that notebook.

## Back again to the example


```{python}
# Array, data frame and plotting libraries.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Statsmodels ANOVA machinery.
import statsmodels.api as sm
import statsmodels.formula.api as smf
```

We return again to our dataset giving amount of weight lost (in kilograms)
after 10 weeks of one of three possible diets.

As in the one-way ANOVA page, we use the `gender` column to define one factor,
and the `diet` column to define another, so we can classify the rows
(individuals) into `Female` or `Male` (`gender`) and into `A`, `B` and `C`
(`diet`).

See: [the dataset page](https://github.com/odsti/datasets/tree/master/sheffield_diet) for more detail.

```{python}
# Read the dataset
diet_data = pd.read_csv('sheffield_diet.csv')
diet_data.head()
```

Pandas `groupby` can classify each row using *both* the `gender` label (level) and the `diet` label (level).  Notice we have different number is the six possible sub-groups.

```{python}
grouped = diet_data.groupby(['gender', 'diet'])
# Show the counts in each of the six groups.
grouped['weight_lost'].count()
```

## General linear model notation

Let us say we want to do an F-test for the interaction term.

We partition the design matrix into the main effect columns and the interaction columns, like this:

```{python}
# Main effects
gender = diet_data['gender']
diet = diet_data['diet']
# Dummy columns for each effect.
g_cols = pd.get_dummies(gender)
d_cols = pd.get_dummies(diet)
# The main effect design matrix.
X1 = np.hstack((g_cols, d_cols))
X1
```

Next we need the design matrix for the interaction terms:

```{python}
# Series giving sub-group labels for each row.
sub_groups = gender.str.cat(diet, sep='-')
# Corresponding design matrix modeling mean for each group.
X2 = pd.get_dummies(sub_groups)
X2
```

Full design:

```{python}
X = np.hstack((X1, X2))
```

The data we are fitting:

```{python}
y = diet_data['weight_lost']
```

Least square parameters for full design:

```{python}
pX = np.linalg.pinv(X)
B = pX @ y
B
```

Fitted y values:

```{python}
y_hat = X @ B
y_hat
```

Errors:

```{python}
e = y - y_hat
e
```

Sum of squared errors:

```{python}
np.sum(e ** 2)
```

Compare to residual sum of squares value in Statsmodel ANOVA table:

```{python}
# Fit
sm_fit = smf.ols('weight_lost ~ gender * diet', data=diet_data).fit()
# Type II (2) sum of squares calculation ANOVA table.
sm.stats.anova_lm(sm_fit, typ=2)
```

We can write that all in one go:

```{python}
e_again = y - X @ pX @ y
np.sum(e_again ** 2)
```

That is also the same calculation as:

```{python}
H = X @ pX
y_hat_again = H @ y
e_3 = (y - y_hat_again)
np.sum(e_3 ** 2)
```

The $H$ matrix above is called the *hat* matrix, because it "puts the hat on"
the y values, meaning, it calculates the best fit predictions for the y values.


In fact, using our matrix algebra, we can write the overall calculation you see in the `e_again` cell above, like this:

```{python}
n = len(y)
# Because:
# y - X @ pX @ y  === y - H @ y === (I - H) @ y
rfm = np.eye(n) - H
e_4 = rfm @ y
np.sum(e_4 ** 2)
```

`rfm` above is the *residual forming matrix*, so called because, as you see, it includes the whole matrix calculation to turn the `y` values into the errors (residuals) with one matrix multiplication.  Multiplying with this matrix extracts all the information that can be modeled by the design matrix `X`.

Notice that `H` and `rfm` *do not depend on `y`* - they depend only on `X`.


Now we calculate the *extra sum of squares* adding the interaction part of the design to a design without the interaction part.

We start by estimating the design without the interaction part - therefore, a design that just models the main effects.

```{python}
# Hat matrix for main effects part of design.
# X1 is the main-effects part of the design.
H_for_X1 = X1 @ np.linalg.pinv(X1)
# Residual forming matrix for X1
rfm1 = np.eye(n) - H_for_X1
# Errors and residual sum of squares for design that
# only includes the main effects.
e1 = rfm1 @ y
RSS1 = np.sum(e1 ** 2)
RSS1
```

```{python}
# Residual sum of squares for full design (X1 with X2)
# We calculated this above, but just for completeness,
# calculate again.
H_for_X = X @ np.linalg.pinv(X)
rfm = np.eye(n) - H_for_X
e = rfm @ y
RSS = np.sum(e ** 2)
RSS
```

```{python}
# Extra sum of squares
ESS = RSS1 - RSS
ESS
```

```{python}
# A reminder of the ANOVA table
# Notice that we have just calculated the ESS for gender:diet
sm.stats.anova_lm(sm_fit, typ=2)
```

Theorem 2 of Computational Statistics and Data Analysis 54 (2010) 1881â€“1893 states that we can also calculate the ESS with:

```{python}
# Applying formula 2 from paper above.
# X2, after removing everything modeled in X1
X_rr = rfm1 @ X2
# Calculate hat matrix for X_rr
pX_rr = np.linalg.pinv(X_rr)
H1 = X_rr @ pX_rr
# Adjusted SNSQGM metric.
y_hat1 = H1 @ y
ESS1_via_y_hat = y @ y_hat1
# We get exactly the same answer as before.
ESS1_via_y_hat
```

The calculation above, gives the same result as the following:

```{python}
np.sum(y_hat1 ** 2)
```

Put another way, the sum of squares of `y * y_hat1` equals the sum of squares of `y_hat1 * y_hat1`.  In fact, this is so for any `y` and `X` in our case of least-squares fitting.  Why?


The sum of squares above is the same operation as the [vector dot product](http://matthew-brett.github.io/teaching/on_vectors.html) of the vector on itself - meaning that the *dot* product is just another way of writing that calculation.  Write `y_hat1` as $\hat{y}$ ($y$ with a hat on top). In mathematics we would write the dot product of $\hat{y}$ on itself as:

$$
\hat{y} . \hat{y}
$$

In code it looks like this:

```{python}
np.dot(y_hat1, y_hat1)
```

Above we discovered that $y . \hat{y} = \hat{y} . \hat{y}$.

This is because we have decomposed $y$ into $\hat{y}$ and a vector of errors (residuals) $e$:

$$
y = X B + e
$$

$$
\hat{y} = X B
$$

$$
y = \hat{y} + e
$$

Because of the least squares fitting for $B$, $e$ is orthogonal to all the columns of $X$, and therefore, $\hat{y}$.

```{python}
e = y - y_hat1
# This is the dot product - (very near) zero if vectors orthogonal
np.dot(e, y_hat1)
```

Therefore, because $y = \hat{y} + e$, and $e . \hat{y} = 0$, by the [algebra of dot products](http://matthew-brett.github.io/teaching/on_vectors.html#distributive-over-vector-addition):

$$
y . \hat{y} = (\hat{y} + e) . \hat{y} = \
\hat{y} . \hat{y} + e . \hat{y} = \hat{y} . \hat{y}
$$



The calculation $\hat{y} . \hat{y}$ above is a direct match to the SNSQGM calculation in the balanced case - the calculation has the effect of squaring the adjusted mean values and multiplying by their respective group counts.
