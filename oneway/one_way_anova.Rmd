---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# One-way ANOVA

Imagine you have three groups, and you want to do a one-level ANOVA to test
for overall differences across the groups.

The general technique for a permutation test is:

* You decide on your metric
* You get your metric for the actual data - observed metric
* You permute your data and take the same metric from the permuted data, and
  repeat many times - fake metrics
* You compare your observed metric to your fake metrics, to see how unusual it
  is.

For a two-sample permutation test, your metric is the difference in the two
sample means.

For a three sample version of the test â€” we need a metric that will be big
where there are big differences between the three groups, and small when there
are small differences.

One good metric could be to:

* Get the sample means for each of the three groups A, B, C, to give `mean_a`,
  `mean_b`, `mean_c`
* Get the mean across all the observations regardless of group
  (`mean_overall`)
* Subtract `mean_overall` from each of `mean_a`, `mean_b`, `mean_c` to give
  `mean_a_diff`, `mean_b_diff`, `mean_c_diff`.

As usual, we need one number as our metric, and as usual, we're interested in
positive as well as negative differences, so we want to get rid of the signs
on these somehow, before adding them up.  One way is to square them, and so
the metric is:

    our_metric = mean_a_diff ** 2 + mean_b_diff ** 2 + mean_c_diff ** 2

This will be big when the individual groups have different means from each
other and small when the means for the groups are pretty similar to each
other, and therefore, to the overall mean.

To follow the recipe above we calculate this metric for the actual groups A,
B, C.   Permute the group labels to give random groups A, B, C, and
recalculate the metric.   See whether the metric in the real data is unusual
in the distribution of the same metric for the permuted groups.

This is the permutation equivalent of the one-way ANOVA.   The one-way ANOVA
just uses some assumptions from the normal distribution to estimate the random
distribution, instead of using permutation to calculate the random
distribution.


## An example

Dataset from <https://dasl.datadescription.com/datafile/contrast-baths>

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

The data corresponds to [this
paper](https://pubmed.ncbi.nlm.nih.gov/19375278/).

From the abstract:

> Methods: Study participants were randomly assigned to one of three treatment
> group protocols--contrast baths with exercise, contrast baths without
> exercise, and an exercise-only control treatment group. Study participants
> were evaluated with hand volumetry, before and after treatment at two
> different data collection periods-pre- and postoperatively.

It is not clear what data we have, but they may be change scores before and
after treatment, for the pre-operative group.

Notice we have three treatment groups, the "Bath" group, the "Bath+Exercise" group and the "Exercise" group.

```{python}
# Read the raw dataset
baths = pd.read_csv('contrast-baths.txt', sep='\t')
baths.head()
```

Both columns are "objects", meaning strings.

```{python}
baths.dtypes
```

In fact, missing data here is a single space character.  Fix that, by replacing single spaces with empty strings, then converting all the "Hand Vol Chg" values to numbers.

```{python}
baths['Hand Vol Chg'] = baths['Hand Vol Chg'].replace(' ', '')
baths['Hand Vol Chg'] = pd.to_numeric(baths['Hand Vol Chg'])
baths.head()
```

For simplicity, drop the missing `NaN` values:

```{python}
clean_baths = baths.dropna()
clean_baths.head()
```

We are now ready to do the analysis.

Here are the data, plotted by group.

```{python}
clean_baths.plot.scatter('Treatment', 'Hand Vol Chg')
```

These are the means for each of the three groups.

```{python}
group_means = clean_baths.groupby('Treatment').mean()
group_means
```

This is the overall mean, ignoring the groups:

```{python}
overall_mean = np.mean(clean_baths['Hand Vol Chg'])
overall_mean
```

Here we plot the data, the group means, and the overall mean on the same plot.

```{python}
clean_baths.plot.scatter('Treatment', 'Hand Vol Chg',
                         label='Data')
plt.scatter(group_means.index, np.array(group_means), color='red',
            label='Group means')
# A dashed line at the overall mean.
plt.plot(group_means.index,
         [overall_mean, overall_mean, overall_mean],
         ':', color='green',
         label='Overall mean')
plt.legend();
```

We calculate the difference between the group means and the overall mean.

```{python}
mean_diffs = group_means - overall_mean
mean_diffs
```

We calculate the sum of squares measure of the difference of the group means from the overall mean.

```{python}
mean_diff_ssq = np.sum(mean_diffs ** 2)
mean_diff_ssq
```

This is our metric.  To make this a bit clearer, we put the calculation of our metric into its own function so we can re-use it on different data frames.

```{python}
def ssq_mean_diffs(df, group_col, val_col):
    overall_mean = np.mean(df[val_col])
    group_means = df.groupby(group_col)[val_col].mean()
    return np.sum((group_means - overall_mean) ** 2)
```

Check that we get the same answer from the function as we did with the step-by-step calculation:

```{python}
ssq_mean_diffs(clean_baths, 'Treatment', 'Hand Vol Chg')
```

Next we consider a single trial in our ideal, null, fake world.  We do this by making a copy of the data frame, and then permuting the Treatment labels, so the association between the Treatment and the change values is random.

```{python}
fake_data = clean_baths.copy()
# Permute the treatment labels
fake_data['Treatment'] = np.random.permutation(fake_data['Treatment'])
fake_data.head()
```

Next we calculate our metric step by step.

```{python}
fake_means = fake_data.groupby('Treatment').mean()
# Notice that the overall_mean cannot change because we did not
# change these values.
fake_ssq = np.sum((fake_means - overall_mean) ** 2)
fake_ssq
```

We can also use our function to do that calculation, to get the same answer:

```{python}
ssq_mean_diffs(fake_data, 'Treatment', 'Hand Vol Chg')
```

Now we are ready to do our simulation.  We do 10000 trials, making a new random association, and recalculating our sum of squares metric.

```{python}
n_iters = 10000
fake_ssqs = np.zeros(n_iters)
for i in np.arange(n_iters):
    fake_data['Treatment'] = np.random.permutation(fake_data['Treatment'])
    fake_ssqs[i] = ssq_mean_diffs(fake_data, 'Treatment', 'Hand Vol Chg')
```

Of course, because these are sums of squares, they must all be positive.

```{python}
plt.hist(fake_ssqs, bins=100);
```

How does our observed sum of squares metric compare to the distribution of fake sum of square metrics?

```{python}
p = np.count_nonzero(fake_ssqs >= float(mean_diff_ssq)) / n_iters
p
```

The p value tells us that this observed metric is very unlikely to have come about in a random world.

## Comparing to standard one-way ANOVA F tests

Notice that we get a similar p value from the standard one-way ANOVA
F-test.  Here is the Statsmodels implementation:

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

# The Q() in the formula is just to allow spaces in the variable name.
mod = smf.ols('Q("Hand Vol Chg") ~ Treatment', data=clean_baths).fit()

sm.stats.anova_lm(mod, typ=1)
```

Here is doing the same calculation in Scipy:

```{python}
from scipy.stats import f_oneway
```

```{python}
# Get the values from the individual groups.
treatment = clean_baths['Treatment']
change = clean_baths['Hand Vol Chg']
bath = change[treatment == 'Bath']
both = change[treatment == 'Bath+Exercise']
exercise = change[treatment == 'Exercise']
```

Do the F-test in Scipy:

```{python}
f_oneway(bath, both, exercise)
```
