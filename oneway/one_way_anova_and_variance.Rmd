---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# One-way anova


Imagine you have three
groups, and you want to do a one-level ANOVA to test for overall
differences across the groups.

The general technique for a permutation test is:

* You decide on your metric
* You get your metric for the actual data - observed metric
* You permute your data and take the same metric from the permuted
data, and repeat many times - fake metrics
* You compare your observed metric to your fake metrics, to see how
unusual it is.

So for a two-sample permutation test, your metric is the difference in
the two sample means.

For a three sample version of the test - we need a metric that will be
big where there are big differences between the three groups, and
small when there are small differences.

One good metric could be to:

Get the sample means for each of the three groups A, B, C, to give
mean_a, mean_b, nean_c

Get the mean across all the observations regardless of group (mean_overall)

Subtract mean_overall from each of mean_a, mean_b, mean_c to give
mean_a_diff, mean_b_diff, mean_c_diff

As usual, we need one number as our metric, and as usual, we're
interested in positive as well as negative differences, so we want to
get rid of the signs on these somehow, before adding them up.  One way
is to square them, and so the metric is:

our_metric = mean_a_diff ** 2 + mean_b_diff ** 2 + mean_c_diff ** 2

This will be big when the individual groups have different means from
each other and small when the means for the groups are pretty similar
to each other, and therefore, to the overall mean.

So - just follow the recipe - calculate this metric for the actual
groups A, B, C.   Permute the group labels to give random groups A, B,
C, and recalculate the metric.   See whether the metric in the real
data is unusual in the distribution of the same metric for the
permuted groups.

This is the permutation equivalent of the one-way ANOVA.   The one-way
ANOVA just uses some assumptions from the normal distribution to
estimate the random distribution, instead of using permutation to
calculate the random distribution.



Dataset from <https://dasl.datadescription.com/datafile/contrast-baths>

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

```{python}
baths = pd.read_csv('contrast-baths.txt', sep='\t')
baths.head()
```

```{python}
baths.dtypes
```

```{python}
baths['Hand Vol Chg'] = baths['Hand Vol Chg'].replace(' ', '')
baths['Hand Vol Chg'] = pd.to_numeric(baths['Hand Vol Chg'])
baths.head()
```

```{python}
baths_clean = baths.dropna()
baths_clean.head()
```

```{python}
baths_clean.plot.scatter('Treatment', 'Hand Vol Chg')
```

```{python}
means = baths_clean.groupby('Treatment').mean()
means
```

```{python}
overall_mean = np.mean(baths_clean['Hand Vol Chg'])
overall_mean
```

```{python}
baths_clean.plot.scatter('Treatment', 'Hand Vol Chg',
                         label='Data')
plt.scatter(means.index, np.array(means), color='red',
            label='Group means')
plt.plot(means.index, [overall_mean, overall_mean, overall_mean],
         ':', color='green',
         label='Overall mean')
plt.legend();
```

```{python}
mean_diffs = means - overall_mean
mean_diffs
```

```{python}
mean_diff_ssq = np.sum(mean_diffs ** 2)
mean_diff_ssq
```

```{python}
n = len(baths_clean)
n
```

```{python}
fake_data = baths_clean.copy()
# Permute the treatment labels
fake_data['Treatment'] = np.random.permutation(fake_data['Treatment'])
fake_data.head()
```

```{python}
fake_means = fake_data.groupby('Treatment').mean()
fake_means
```

```{python}
# The overall mean cannot change, because we're putting in the same
# set of numbers, perhaps in a different order.
np.mean(fake_data['Hand Vol Chg']) == overall_mean
```

```{python}
fake_ssq = np.sum((fake_means - overall_mean) ** 2)
fake_ssq
```

```{python}
def ssq_mean_diffs(df, group_col, val_col):
    overall_mean = np.mean(df[val_col])
    group_means = df.groupby(group_col)[val_col].mean()
    return np.sum((group_means - overall_mean) ** 2)
```

```{python}
ssq_mean_diffs(baths_clean, 'Treatment', 'Hand Vol Chg')
```

```{python}
ssq_mean_diffs(fake_data, 'Treatment', 'Hand Vol Chg')
```

```{python}
n_iters = 10000
fake_ssqs = np.zeros(n_iters)
for i in np.arange(n_iters):
    fake_data['Treatment'] = np.random.permutation(fake_data['Treatment'])
    fake_ssqs[i] = ssq_mean_diffs(fake_data, 'Treatment', 'Hand Vol Chg')
```

```{python}
plt.hist(fake_ssqs, bins=100);
```

```{python}
p = np.count_nonzero(fake_ssqs >= float(mean_diff_ssq)) / n_iters
p
```

Notice the all-but-identical value from the standard one-way ANOVA F-test:

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

mod = smf.ols('Q("Hand Vol Chg") ~ Treatment', data=baths_clean).fit()

sm.stats.anova_lm(mod, typ=1) 
```

Or:

```{python}
from scipy.stats import f_oneway
```

```{python}
treatment = baths_clean['Treatment']
change = baths_clean['Hand Vol Chg']
bath = change[treatment == 'Bath']
both = change[treatment == 'Bath+Exercise']
exercise = change[treatment == 'Exercise']
f_oneway(bath, both, exercise)
```

Thinking of the residual variances:

```{python}
bath_resid = bath - np.mean(bath)
both_resid = both - np.mean(both)
ex_resid = exercise - np.mean(exercise)
# Stick these all together into one array
all_group_resids = np.concatenate([bath_resid, both_resid, ex_resid])
ssq_groups = np.sum(all_group_resids ** 2)
ssq_groups
```

```{python}
ssq_overall = np.sum((change - overall_mean) ** 2)
ssq_overall
```

```{python}
F = ((ssq_overall - ssq_groups) / 2) / (ssq_groups / (n - 3))
F
```

```{python}
(ssq_overall - ssq_groups) / 2 
```

```{python}
top = ssq_mean_diffs(baths_clean, 'Treatment', 'Hand Vol Chg')
```

```{python}
x = top / F
x
```

```{python}

```

```{python}

```
